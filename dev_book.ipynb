{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "D = [monitor_sml[:,2], monitor_big[:,2]]\n",
    "b = ax.boxplot(D)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "ax.set_xticklabels(['BRS small params', 'BRS big params'])\n",
    "\n",
    "# plt.title('Reward: 1-small values; 2-big values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD = 3 #1e-3\n",
    "V_RANGE = (1, 10, 1)\n",
    "N_SAMPLE_RANGE = (1, N_ROLLOUTS, 1)\n",
    "\n",
    "env = gym.make('Pendulum-v1', g=9.82)\n",
    "n_action = 1 \n",
    "n_state = 3\n",
    "\n",
    "monitor_rs = []\n",
    "saved_polices = []\n",
    "for n_samples in tqdm(np.arange(*N_SAMPLE_RANGE)):\n",
    "    for v in tqdm(np.arange(*V_RANGE), leave=False):\n",
    "        model = BRS_RS(n_state, n_action, std=STD, v=v, n_samples=n_samples)\n",
    "        \n",
    "        trained_model = train(env, model, STD, v)\n",
    "        score = evaluate(env, trained_model)\n",
    "        \n",
    "        monitor_rs.append([n_samples, v, score])\n",
    "        saved_polices += [model.policy.M]\n",
    "\n",
    "monitor_rs = np.array(monitor_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(monitor_rs[:,0], monitor_rs[:,1], monitor_rs[:,2], c=monitor_rs[:,2], s=45, alpha=1)\n",
    "# ax.view_init(10, 90)\n",
    "ax.set_xlabel('Num of samples per update')\n",
    "ax.set_ylabel('V')\n",
    "ax.set_zlabel('Reward')\n",
    "# ax.plot(monitor_rs[:,0], monitor_rs[:,1], monitor_rs[:,2], color='k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "D = [monitor_sml[:,2], monitor_big[:,2], monitor_rs[:,2]]\n",
    "b = ax.boxplot(D)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "ax.set_xticklabels(['BRS small params', 'BRS big params', 'BRS-RS'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD = 1e-3\n",
    "V_RANGE = (1, 10, 1)\n",
    "N_SAMPLE_RANGE = (1, N_ROLLOUTS, 1)\n",
    "\n",
    "env = gym.make('Pendulum-v1', g=9.82)\n",
    "n_action = 1 \n",
    "n_state = 3\n",
    "\n",
    "monitor_ss = []\n",
    "#saved_polices = []\n",
    "for n_samples in tqdm(np.arange(*N_SAMPLE_RANGE)):\n",
    "    for v in tqdm(np.arange(*V_RANGE), leave=False):\n",
    "        model = BRS_SS(n_state, n_action, std=STD, v=v, n_samples=n_samples)\n",
    "        \n",
    "        trained_model = train(env, model, STD, v)\n",
    "        score = evaluate(env, trained_model)\n",
    "        \n",
    "        monitor_ss.append([n_samples, v, score])\n",
    "        saved_polices += [model.policy.M]\n",
    "\n",
    "monitor_ss = np.array(monitor_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7321c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(monitor_rs[:,0], monitor_ss[:,1], monitor_ss[:,2], c=monitor_ss[:,2], s=45, alpha=1)\n",
    "# ax.view_init(10, 90)\n",
    "ax.set_xlabel('Rollouts')\n",
    "ax.set_ylabel('V')\n",
    "ax.set_zlabel('Reward')\n",
    "# ax.plot(monitor_rs[:,0], monitor_rs[:,1], monitor_rs[:,2], color='k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91292472",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "D = [monitor_sml[:,2], monitor_big[:,2], monitor_rs[:,2], monitor_ss[:,2]]\n",
    "\n",
    "box_plot = ax.boxplot(D)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "ax.set_xticklabels(['BRS small params', 'BRS big params', 'BRS-RS', 'BRS-SS'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARS:\n",
    "    def __init__(self, n_state, n_action, b=2, alpha=1e-3, std=1, v=1e-3,\n",
    "                state_high=None, state_low=None):\n",
    "        self.policy = Linear_model(n_state, n_action)\n",
    "        self.buffer = []\n",
    "        self.alpha = alpha\n",
    "        self.std = std\n",
    "        self.v = v\n",
    "        # b - number of b top-performing directions \n",
    "        self.b = b\n",
    "        \n",
    "        self.state_low = state_low \n",
    "        self.state_high = state_high\n",
    "        \n",
    "    def learn(self):\n",
    "        # sort noise permutations by reward they achive, \n",
    "        # and select only high reward ones\n",
    "        b_buffer = self.sort_directions()\n",
    "        \n",
    "        reward_std = b_buffer[:, -2:].std()\n",
    "        norm = self.alpha / (self.b * reward_std)\n",
    "        \n",
    "        reward_sum = 0\n",
    "        for step in b_buffer:\n",
    "            # ( R+ - R- ) * noise\n",
    "            reward_sum += (step[-2] - step[-1]) * step[-3]\n",
    "    \n",
    "        self.policy.M = self.policy.M + (norm * reward_sum)\n",
    "        \n",
    "        # reset buffer \n",
    "        self.buffer = []\n",
    "    \n",
    "    def sort_directions(self):\n",
    "        buffer = np.array(self.buffer, dtype=object)\n",
    "        b_rewards = buffer[:, -2:].sum(-1)\n",
    "        \n",
    "        # idxs from low to high\n",
    "        b_idxs = np.argsort(b_rewards)\n",
    "        b_buffer = buffer[b_idxs][-self.b:]\n",
    "        \n",
    "        # print(buffer[b_idxs][:-self.b][:,-2:].mean(), buffer[b_idxs][-self.b:][:,-2:].mean())\n",
    "        return b_buffer\n",
    "    \n",
    "    def get_actions(self, state, is_train=False):\n",
    "        if not self.state_high is None:\n",
    "            # MINMAX NORM {value - min}/{max - min}\n",
    "            state = (state - self.state_low)/(self.state_high - self.state_low)\n",
    "            \n",
    "        if is_train:\n",
    "            noise = np.random.normal(0,self.std, size=self.policy.M.shape)\n",
    "            pos_action = self.policy(state,  self.v, noise)\n",
    "            neg_action = self.policy(state, -self.v, noise)\n",
    "            \n",
    "            pos_action = self._remap_actions(pos_action)\n",
    "            neg_action = self._remap_actions(neg_action)\n",
    "            return pos_action, neg_action, noise\n",
    "        else:\n",
    "            action = self.policy(state)\n",
    "            return self._remap_actions(action)\n",
    "            \n",
    "    \n",
    "    def remember(self, memory):\n",
    "        self.buffer.append(memory)\n",
    "        \n",
    "    def _remap_actions(self, action):\n",
    "        return 2 * np.tanh(action)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b09df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v1', g=9.82)\n",
    "env.seed(SEED)\n",
    "observation = env.reset()\n",
    "\n",
    "state_high = env.observation_space.high\n",
    "state_low = env.observation_space.low\n",
    "model = ARS(3, 1, b=N_ROLLOUTS, std=1, \n",
    "            state_low=state_low, state_high=state_high)\n",
    "\n",
    "REWARDS = []\n",
    "t_reward = 0\n",
    "for step in tqdm(range(TOTAL_STEPS)):\n",
    "    #env.render()\n",
    "    \n",
    "    for ro in range(N_ROLLOUTS):\n",
    "        pos_action, neg_action, noise = model.get_actions(observation, is_train=True)\n",
    "        pos_observation, pos_reward, pos_done, pos_info = env.step(pos_action)\n",
    "        neg_observation, neg_reward, neg_done, neg_info = env.step(neg_action)\n",
    "\n",
    "        model.remember([pos_action[0], neg_action[0], noise, pos_reward, neg_reward])\n",
    "        t_reward += (pos_reward + neg_reward)/2\n",
    "        observation = env.reset()\n",
    "        #observation = pos_observation if np.random.rand()>0.5 else neg_observation\n",
    "    \n",
    "    # if (pos_done or neg_done):\n",
    "        \n",
    "        \n",
    "    # if (step % N_ROLLOUTS == 0 and step != 0):\n",
    "    model.learn()\n",
    "    REWARDS.append(t_reward)\n",
    "    t_reward = 0\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6882fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(REWARDS)\n",
    "plt.title(np.mean(REWARDS).round(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_collector = {''}\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v1', g=9.81)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "env.seed(SEED)\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "state_high = env.observation_space.high\n",
    "state_low = env.observation_space.low\n",
    "\n",
    "# EVALUATION\n",
    "# env = gym.make('Pendulum-v1', g=9.82)\n",
    "# observation = env.reset()\n",
    "\n",
    "score = 0\n",
    "for step in tqdm(range(EVAL_STEPS)):\n",
    "    #env.render()\n",
    "    \n",
    "    action = model.get_actions(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    score += reward\n",
    "    \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        \n",
    "env.close()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d66fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ARS(3, 1, b=N_ROLLOUTS, std=1, \n",
    "#             state_low=state_low, state_high=state_high)\n",
    "\n",
    "# REWARDS = []\n",
    "# t_reward = 0\n",
    "# for step in tqdm(range(TOTAL_STEPS)):\n",
    "#     #env.render()\n",
    "    \n",
    "#     for ro in range(N_ROLLOUTS):\n",
    "#         pos_action, neg_action, noise = model.get_actions(observation, is_train=True)\n",
    "#         pos_observation, pos_reward, pos_done, pos_info = env.step(pos_action)\n",
    "#         neg_observation, neg_reward, neg_done, neg_info = env.step(neg_action)\n",
    "\n",
    "#         model.remember([pos_action[0], neg_action[0], noise, pos_reward, neg_reward])\n",
    "#         t_reward += (pos_reward + neg_reward)/2\n",
    "#         #observation = env.reset()\n",
    "#         observation = pos_observation if np.random.rand()>0.5 else neg_observation\n",
    "\n",
    "#     model.learn()\n",
    "#     REWARDS.append(t_reward)\n",
    "#     t_reward = 0\n",
    "    \n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.policy(np.array([1,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c917bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5053ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
