{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c52f3d-7dc8-4d16-b1cc-771e42ba2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ca7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a914dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aed910-42cf-4d73-82cb-231817699486",
   "metadata": {},
   "source": [
    "Finished:\n",
    "- Baseline BRS with large parameters range: **std & v - \\[1e-3, 1\\]** (all buffer)\n",
    "- Baseline BRS with small parameters range **std & v\\[1e-3, 10\\]** (all buffer)\n",
    "- Baseline BRS with random sampling (n_samples random samples form buffer) `std=1e-3`\n",
    "- Baseline BRS with sorted sampling (n_samples sorted samples form buffer)\n",
    "\n",
    "TODO:\n",
    "List of things to check:\n",
    "- VARIABLES:\n",
    "    - Random seed vs total_mean_reward(TMR)\n",
    "    - N_ROLLOUTS vs TMR\n",
    "    - TOTAL_STEPS vs TMR\n",
    "    - b=2\n",
    "    - alpha=1e-3 \n",
    "    - std=1\n",
    "    - v=1e-3\n",
    "    - States norm (minmax)\n",
    "\n",
    "COMPONENTS:\n",
    "- sample number unsorted vs sorted by reward \n",
    "- rollouts\n",
    "- input data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8fd775-1f0a-4ce7-8ae5-ec2742a9bfd6",
   "metadata": {},
   "source": [
    "# Hyper Parameters and Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734ad0f4-b99c-4243-a61f-8b121838233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Random seed \n",
    "SEED = 42\n",
    "# Number of steps to wait until next update \n",
    "N_ROLLOUTS = 100\n",
    "# Number of total steps of env simulation\n",
    "TOTAL_STEPS = 1000\n",
    "# Number of evaluation stapes \n",
    "EVAL_STEPS = 1000\n",
    "#\n",
    "# ALPHA = 1e-3 \n",
    "# STD for normal dist. for exploration noise\n",
    "# STD = 1\n",
    "# Amount of noise we add (weight multiplyer)\n",
    "# V = 1e-3\n",
    "\n",
    "# Parameters ranges\n",
    "# (min, max, step)\n",
    "# SEED_RANGE = (0, 1024, 1)\n",
    "# TOTAL_STEPS_RANGE = (1, 1e+5, 1)\n",
    "# N_ROLLOUTS_RANGE = (0, TOTAL_STEPS_RANGE[1], 1)\n",
    "# ALPHA_RANGE = (0, 1, 1e-3)\n",
    "# STD_RANGE = (0, 1, 1e-3)\n",
    "# V_RANGE = (0, 1, 1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d70c8-cade-470d-bd48-b7a65a68411b",
   "metadata": {},
   "source": [
    "# Model\n",
    "In all experiments will be used one type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567b9802-4a4f-4c25-b095-1ad791c1c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_model:\n",
    "    def __init__(self, n_state, n_action): \n",
    "        # M stands for parameters of this model\n",
    "        self.M = np.zeros((n_action, n_state))\n",
    "        \n",
    "    def __call__(self, state, noise=0):\n",
    "        action = (self.M + noise) @ state\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f08e7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_model_v2:\n",
    "    def __init__(self, n_state, n_action): \n",
    "        # M stands for parameters of this model\n",
    "        self.M = np.zeros((n_action, n_state))\n",
    "        self.mu = np.zeros((n_state))\n",
    "        self.sigma = np.ones(n_state)\n",
    "        \n",
    "    def __call__(self, state, noise=0):\n",
    "        # [(n_action, n_state) + (n_action, n_state)] @ (n_state,n_state) @ [(n_state, 1) - (n_state, 1)]\n",
    "        _sigma = np.diag(self.sigma)\n",
    "        action = (self.M + noise) @ _sigma @ (state - self.mu)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4224be2",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8449a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v1    (V1 model with simple update)\n",
    "'''\n",
    "class BaseRandomSearch:\n",
    "    def __init__(self, n_state, n_action, std):\n",
    "        self.policy = Linear_model(n_state,n_action)\n",
    "        self.std = std\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def get_actions(self, state, is_train=False):\n",
    "        if is_train:\n",
    "            noise = np.random.normal(0, self.std, size=self.policy.M.shape)\n",
    "            action_pos = self.policy(state,  noise)\n",
    "            action_neg = self.policy(state, -noise)\n",
    "            action_pos = self._remap_actions(action_pos)\n",
    "            action_neg = self._remap_actions(action_neg)\n",
    "            return action_pos, action_neg, noise\n",
    "        else:\n",
    "            action = self.policy(state)\n",
    "            return self._remap_actions(action)\n",
    "        \n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        for step in self.buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update  \n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def remember(self, memory):\n",
    "        self.buffer.append(memory)\n",
    "        \n",
    "    def _remap_actions(self, action):\n",
    "        return np.tanh(action)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29aaa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v1_RS (V1 model with random subset buffer update)\n",
    "'''\n",
    "class BRS_RS(BaseRandomSearch):\n",
    "    def __init__(self, n_state, n_action, std, n_samples):\n",
    "        self.policy = Linear_model(n_state,n_action)\n",
    "        self.std = std\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        buffer_idxs = np.random.choice(np.arange(len(self.buffer)), self.n_samples)\n",
    "        buffer = np.array(self.buffer, dtype=object)[buffer_idxs]\n",
    "        \n",
    "        for step in buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update  \n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f7e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v1_SS (V1 model with sorted subset buffer update)\n",
    "'''\n",
    "class BRS_SS(BRS_RS):\n",
    "    def _sort_directions(self):\n",
    "        buffer = np.array(self.buffer, dtype=object)\n",
    "        b_rewards = buffer[:, -2:].sum(-1)\n",
    "\n",
    "        # idxs from low to high\n",
    "        b_idxs = np.argsort(b_rewards)\n",
    "        b_buffer = buffer[b_idxs][-self.n_samples:]\n",
    "\n",
    "        return b_buffer\n",
    "\n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        buffer = self._sort_directions()\n",
    "        \n",
    "        for step in buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update  \n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f96af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v2    (V2 model with simple update)\n",
    "'''\n",
    "class BaseRandomSearch_v2:\n",
    "    def __init__(self, n_state, n_action, std):\n",
    "        self.policy = Linear_model_v2(n_state,n_action)\n",
    "        self.std = std\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.states = []\n",
    "        \n",
    "    def get_actions(self, state, is_train=False):\n",
    "        if is_train:\n",
    "            noise = np.random.normal(0, self.std, size=self.policy.M.shape)\n",
    "            action_pos = self.policy(state,  noise)\n",
    "            action_neg = self.policy(state, -noise)\n",
    "            action_pos = self._remap_actions(action_pos)\n",
    "            action_neg = self._remap_actions(action_neg)\n",
    "            return action_pos, action_neg, noise\n",
    "        else:\n",
    "            action = self.policy(state)\n",
    "            return self._remap_actions(action)\n",
    "        \n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        for step in self.buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update  \n",
    "        \n",
    "        self.policy.mu = np.mean(self.states, 0)\n",
    "        self.policy.sigma = np.std(self.states, 0)\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def remember(self, memory):\n",
    "        # pos_action, neg_action, noise, pos_reward, neg_reward\n",
    "        self.buffer.append(memory)\n",
    "        \n",
    "    def _remap_actions(self, action):\n",
    "        return np.tanh(action)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "758f9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v2_RS (V2 model with random subset buffer update)\n",
    "'''\n",
    "class BRS_v2_RS(BaseRandomSearch_v2):\n",
    "    def __init__(self, n_state, n_action, std, n_samples):\n",
    "        self.policy = Linear_model_v2(n_state,n_action)\n",
    "        self.std = std\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.states = []\n",
    "        \n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        buffer_idxs = np.random.choice(np.arange(len(self.buffer)), self.n_samples)\n",
    "        buffer = np.array(self.buffer, dtype=object)[buffer_idxs]\n",
    "        \n",
    "        for step in buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update \n",
    "        \n",
    "        self.policy.mu = np.mean(self.states, 0)\n",
    "        self.policy.sigma = np.std(self.states, 0)\n",
    "        \n",
    "        self.buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2a1a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BRS_v2_SS (V2 model with sorted subset buffer update)\n",
    "'''\n",
    "class BRS_v2_SS(BRS_v2_RS):\n",
    "    def _sort_directions(self):\n",
    "        buffer = np.array(self.buffer, dtype=object)\n",
    "        b_rewards = buffer[:, -2:].sum(-1)\n",
    "\n",
    "        # idxs from low to high\n",
    "        b_idxs = np.argsort(b_rewards)\n",
    "        b_buffer = buffer[b_idxs][-self.n_samples:]\n",
    "\n",
    "        return b_buffer\n",
    "\n",
    "    def learn(self):\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        buffer = self._sort_directions()\n",
    "        \n",
    "        for step in buffer:\n",
    "            update += ((step[-2] - step[-1]) * step[-3]) / self.std\n",
    "        \n",
    "        self.policy.M = self.policy.M + update\n",
    "        \n",
    "        self.policy.mu = np.mean(self.states, 0)\n",
    "        self.policy.sigma = np.std(self.states, 0)\n",
    "        \n",
    "        self.buffer = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d28798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ARS_v1\n",
    "'''\n",
    "class ARS_v1:\n",
    "    def __init__(self, n_state, n_action, std, n_samples, alpha=1e-3, state_low=None, state_high=None):\n",
    "        self.policy = Linear_model(n_state,n_action)\n",
    "        self.std = std\n",
    "        self.alpha = alpha\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "        self.state_low=state_low\n",
    "        self.state_high=state_high\n",
    "        \n",
    "    def get_actions(self, state, is_train=False):\n",
    "        if (self.state_high is not None and self.state_low is not None):\n",
    "            # MINMAX NORM {value - min}/{max - min}\n",
    "            state = (state - self.state_low)/(self.state_high - self.state_low)\n",
    "    \n",
    "        if is_train:\n",
    "            noise = np.random.normal(0, self.std, size=self.policy.M.shape)\n",
    "            action_pos = self.policy(state,  noise)\n",
    "            action_neg = self.policy(state, -noise)\n",
    "            action_pos = self._remap_actions(action_pos)\n",
    "            action_neg = self._remap_actions(action_neg)\n",
    "            return action_pos, action_neg, noise\n",
    "        else:\n",
    "            action = self.policy(state)\n",
    "            return self._remap_actions(action)\n",
    "    \n",
    "    def _sort_directions(self):\n",
    "        buffer = np.array(self.buffer, dtype=object)\n",
    "        b_rewards = buffer[:, -2:].sum(-1)\n",
    "\n",
    "        # idxs from low to high\n",
    "        b_idxs = np.argsort(b_rewards)\n",
    "        b_buffer = buffer[b_idxs][-self.n_samples:]\n",
    "\n",
    "        return b_buffer\n",
    "\n",
    "    def learn(self):\n",
    "        b_buffer = self._sort_directions()\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        for step in b_buffer:\n",
    "            r_p = step[-2]\n",
    "            r_n = step[-1]\n",
    "            noise = step[-3]\n",
    "            update += ((r_p - r_n) * noise)\n",
    "        \n",
    "        reward_std = b_buffer[:, -2:].std()\n",
    "        norm = self.alpha / (self.n_samples * reward_std)\n",
    "        \n",
    "        self.policy.M = self.policy.M + (norm * update)\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "    def remember(self, memory):\n",
    "        # pos_action, neg_action, noise, pos_reward, neg_reward\n",
    "        self.buffer.append(memory)\n",
    "        \n",
    "    def _remap_actions(self, action):\n",
    "        return np.tanh(action)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e5456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ARS_v2\n",
    "'''\n",
    "class ARS_v2(ARS_v1):\n",
    "    def __init__(self, n_state, n_action, std, n_samples, alpha=1e-3, state_low=None, state_high=None):\n",
    "        self.policy = Linear_model_v2(n_state,n_action)\n",
    "        self.std = std\n",
    "        self.alpha = alpha\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        self.state_low=state_low\n",
    "        self.state_high=state_high\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.states = []\n",
    "\n",
    "    def learn(self):\n",
    "        b_buffer = self._sort_directions()\n",
    "        update = np.zeros_like(self.policy.M)\n",
    "        \n",
    "        for step in b_buffer:\n",
    "            r_p = step[-2]\n",
    "            r_n = step[-1]\n",
    "            noise = step[-3]\n",
    "            update += ((r_p - r_n) * noise)\n",
    "        \n",
    "        reward_std = b_buffer[:, -2:].std()\n",
    "        if (self.state_high is not None and self.state_low is not None):\n",
    "            # MINMAX NORM {value - min}/{max - min}\n",
    "            reward_std = (reward_std - self.state_low)/(self.state_high - self.state_low)\n",
    "        \n",
    "        norm = self.alpha / (self.n_samples * reward_std)\n",
    "        \n",
    "        self.policy.M = self.policy.M + (norm * update)\n",
    "        \n",
    "        self.policy.mu = np.mean(self.states, 0)\n",
    "        self.policy.sigma = np.std(self.states, 0)\n",
    "        \n",
    "        self.buffer = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29024989",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1013b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, t_model, total_steps, n_rollouts, tseed, version='v1'):\n",
    "    np.random.seed(tseed)\n",
    "    try:\n",
    "        observation = env.reset(seed=tseed)\n",
    "    except:\n",
    "        env.seed(tseed)\n",
    "        observation = env.reset()\n",
    "        \n",
    "    for step in range(total_steps):\n",
    "        for ro in range(n_rollouts):\n",
    "            pos_action, neg_action, noise = t_model.get_actions(observation, is_train=True)\n",
    "            pos_observation, pos_reward, pos_done, pos_info = env.step(pos_action)\n",
    "            neg_observation, neg_reward, neg_done, neg_info = env.step(neg_action)\n",
    "            \n",
    "            \n",
    "            if version == 'v1':\n",
    "                t_model.remember([pos_action[0], neg_action[0], noise[0], pos_reward, neg_reward])\n",
    "            elif version == 'v2':\n",
    "                t_model.states += pos_observation, neg_observation\n",
    "                t_model.remember([pos_action[0], neg_action[0], noise[0], pos_reward, neg_reward])\n",
    "            else:\n",
    "                print(0/0)\n",
    "                \n",
    "            coin = np.random.rand() \n",
    "#             if coin <= 0.05:\n",
    "#                 observation = env.reset()\n",
    "            if coin < 0.5:\n",
    "                observation = pos_observation \n",
    "            elif coin >= 0.5:\n",
    "                observation = neg_observation\n",
    "                \n",
    "            if (pos_done or neg_done):\n",
    "                observation = env.reset()\n",
    "            \n",
    "        t_model.learn()\n",
    "\n",
    "    env.close()\n",
    "    return t_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7615abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, model, eval_steps, eseed):\n",
    "    np.random.seed(eseed)\n",
    "    try:\n",
    "        observation = env.reset(seed=eseed)\n",
    "    except:\n",
    "        env.seed(eseed)\n",
    "        observation = env.reset()\n",
    "        \n",
    "    score = 0\n",
    "\n",
    "    for step in range(eval_steps):\n",
    "        action = model.get_actions(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            observation = env.reset()\n",
    "\n",
    "    env.close()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24bbcda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_4_params(STD_RANGE, V_RANGE, TOTAL_STEPS, N_ROLLOUTS, SEED, EVAL_STEPS):\n",
    "    env = gym.make('Pendulum-v1', g=9.81)\n",
    "    n_action = 1 \n",
    "    n_state = 3\n",
    "\n",
    "    monitor = []\n",
    "    saved_polices = []\n",
    "    for std in tqdm(np.arange(*STD_RANGE)):\n",
    "        for v in tqdm(np.arange(*V_RANGE), leave=False):\n",
    "            model = BaseRandomSearch(n_state, n_action, std=std, v=v)\n",
    "\n",
    "            trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED)\n",
    "            score = evaluate(env, trained_model,EVAL_STEPS, SEED)\n",
    "            monitor.append([std, v, score])\n",
    "            saved_polices += [model.policy.M]\n",
    "    monitor = np.array(monitor)\n",
    "    return monitor, saved_polices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22c7c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_std__v_r(monitor):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.scatter3D(monitor[:,0], monitor[:,1], monitor[:,2], c=monitor[:,2], cmap='viridis_r')\n",
    "\n",
    "    ax.set_xlabel('STD')\n",
    "    ax.set_ylabel('V')\n",
    "    ax.set_zlabel('Reward')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a139a2-7515-4310-b799-463d68e3bf9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ablation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b3e44-e553-4c86-863e-fe08c9f25c57",
   "metadata": {},
   "source": [
    "Train BRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d46319cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STD_RANGE = (1e-3, 1, 8e-2)\n",
    "# V_RANGE = (1e-3, 1, 8e-2)\n",
    "# monitor_1, sp1 = search_4_params(STD_RANGE, V_RANGE, TOTAL_STEPS, N_ROLLOUTS, SEED, EVAL_STEPS)\n",
    "\n",
    "# STD_RANGE = (1e-3, 0.02, 1e-3)\n",
    "# V_RANGE = (1, 110, 10)\n",
    "# monitor_2, sp2 = search_4_params(STD_RANGE, V_RANGE, TOTAL_STEPS, N_ROLLOUTS, SEED, EVAL_STEPS)\n",
    "\n",
    "# STD_RANGE = (1e-3, 0.002, 1e-4)\n",
    "# V_RANGE = (10, 1100, 100)\n",
    "# monitor_3, sp3 = search_4_params(STD_RANGE, V_RANGE, TOTAL_STEPS, N_ROLLOUTS, SEED, EVAL_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96094bd7-b420-4453-a507-869ab58addd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_std__v_r(monitor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acbc97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_std__v_r(monitor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6873ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_std__v_r(monitor_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acdbf367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# D = [monitor_1[:,2], monitor_2[:,2], monitor_3[:,2]]\n",
    "# b = ax.boxplot(D)\n",
    "\n",
    "# ax.set_xlabel('Model')\n",
    "# ax.set_ylabel('Reward')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c4da9",
   "metadata": {},
   "source": [
    "Now we showed that optimal values for parameters on simple BRS model are:  \n",
    "```\n",
    "STD = 1.5e-3\n",
    "V = 100\n",
    "```  \n",
    "We can proceed further with those fixed, for reprodusability and purposes of fair comparison.  \n",
    "Also we can check differences between policies learn by each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "611a3736-b6ac-407b-9510-5fd6d975c007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Sample of learned polices:')\n",
    "# print('Monitor 1:')\n",
    "# [print(v) for v in sp1[:3]]\n",
    "# print('Monitor 2:')\n",
    "# [print(v) for v in sp2[:3]]\n",
    "# print('Monitor 3:')\n",
    "# [print(v) for v in sp3[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebad5f-db0e-4e39-9d9f-c597669420b1",
   "metadata": {},
   "source": [
    "# Actual models testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd3c746b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym.envs.box2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m     11\u001b[0m RANDOM_EVAL_SEEDS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m10000\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m n_action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m n_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jn/lib/python3.10/site-packages/gym/envs/registration.py:590\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jn/lib/python3.10/site-packages/gym/envs/registration.py:56\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/miniconda3/envs/jn/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym.envs.box2d'"
     ]
    }
   ],
   "source": [
    "STD = 1\n",
    "TOTAL_STEPS = 100\n",
    "N_ROLLOUTS = 100 \n",
    "N_SAMPLES_UPDATE = N_ROLLOUTS//2\n",
    "\n",
    "env_id = 1\n",
    "envs = ['Pendulum-v1', 'LunarLanderContinuous-v2']\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "RANDOM_EVAL_SEEDS = np.random.randint(10000, size=50)\n",
    "\n",
    "env = gym.make(envs[env_id])\n",
    "\n",
    "n_action = env.action_space.shape[0]\n",
    "n_state = env.observation_space.shape[0]\n",
    "print(n_action, n_state)\n",
    "\n",
    "state_high = env.observation_space.high\n",
    "state_low = env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "809c18ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (0.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (from gym[box2d]) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (from gym[box2d]) (1.22.3)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (from gym[box2d]) (0.0.7)\n",
      "Requirement already satisfied: pygame==2.1.0 in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (from gym[box2d]) (2.1.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/veax-void/miniconda3/envs/jn/lib/python3.10/site-packages (from gym[box2d]) (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ace8e7",
   "metadata": {},
   "source": [
    "## BRS_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f796d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BaseRandomSearch(n_state, n_action, std=STD)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED)\n",
    "\n",
    "BRS_v1_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v1_rewards += [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a570de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(BRS_v1_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757676f",
   "metadata": {},
   "source": [
    "## BRS_v1_RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BRS_RS(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED)\n",
    "\n",
    "BRS_v1_RS_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v1_RS_rewards += [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b7e0f",
   "metadata": {},
   "source": [
    "## BRS_v1_SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BRS_SS(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED)\n",
    "\n",
    "BRS_v1_SS_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v1_SS_rewards += [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c761a9",
   "metadata": {},
   "source": [
    "## BRS_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977eb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseRandomSearch_v2(n_state, n_action, std=STD)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v2')\n",
    "\n",
    "BRS_v2_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v2_rewards += [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c423a0e",
   "metadata": {},
   "source": [
    "## BRS_v2_RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983379de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BRS_v2_RS(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v2')\n",
    "\n",
    "BRS_v2_RS_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v2_RS_rewards += [score]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cffe11",
   "metadata": {},
   "source": [
    "## BRS_v2_SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BRS_v2_SS(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v2')\n",
    "\n",
    "BRS_v2_SS_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    BRS_v2_SS_rewards += [score]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39711b1",
   "metadata": {},
   "source": [
    "## ARS_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS_v1(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v1')\n",
    "\n",
    "ARS_v1_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    ARS_v1_rewards += [score]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd53c21",
   "metadata": {},
   "source": [
    "## ARS_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS_v2(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v2')\n",
    "\n",
    "ARS_v2_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    ARS_v2_rewards += [score]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b01406",
   "metadata": {},
   "source": [
    "## ARS_v1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS_v1(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE,\n",
    "              state_low=state_low, state_high=state_high)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v1')\n",
    "\n",
    "ARS_v1_norm_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    ARS_v1_norm_rewards += [score]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac941f",
   "metadata": {},
   "source": [
    "## ARS_v2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS_v2(n_state, n_action, std=STD, n_samples=N_SAMPLES_UPDATE,\n",
    "              state_low=state_low, state_high=state_high)\n",
    "\n",
    "trained_model = train(env, model, TOTAL_STEPS, N_ROLLOUTS, SEED, version='v2')\n",
    "\n",
    "ARS_v2_norm_rewards = []\n",
    "for eseed in tqdm(RANDOM_EVAL_SEEDS):\n",
    "    score = evaluate(env, trained_model, EVAL_STEPS, int(eseed))\n",
    "    ARS_v2_norm_rewards += [score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a16cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = [BRS_v1_rewards, BRS_v1_RS_rewards, BRS_v1_SS_rewards, \n",
    "     BRS_v2_rewards, BRS_v2_RS_rewards, BRS_v2_SS_rewards,\n",
    "     ARS_v1_rewards, ARS_v2_rewards,\n",
    "     ARS_v1_norm_rewards, ARS_v2_norm_rewards]\n",
    "\n",
    "labels = ['BRS_v1', 'BRS_v1_RS', 'BRS_v1_SS', \n",
    "          'BRS_v2', 'BRS_v2_RS', 'BRS_v2_SS',\n",
    "          'ARS_v1', 'ARS_v2',\n",
    "          'ARS_v1_norm', 'ARS_v2_norm']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i, d in enumerate(D):\n",
    "    plt.plot(d, '.-', label=labels[i])\n",
    "\n",
    "    \n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Evaluation number')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7169e47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "b = ax.boxplot(D)\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Evaluation reward scores, for tested models')\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd028da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
